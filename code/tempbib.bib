
@techreport{hutter_kernel_2013,
	type = {{arXiv} e-print},
	title = {A Kernel for Hierarchical Parameter Spaces},
	url = {http://arxiv.org/abs/1310.5738},
	abstract = {We define a family of kernels for mixed continuous/discrete hierarchical parameter spaces and show that they are positive definite.},
	number = {1310.5738},
	urldate = {2013-10-24},
	author = {Hutter, Frank and Osborne, Michael A.},
	month = oct,
	year = {2013},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
},

@techreport{pokarowski_combined_2013,
	type = {{arXiv} e-print},
	title = {Combined l\_1 and greedy l\_0 penalized least squares for linear model selection},
	url = {http://arxiv.org/abs/1310.6062},
	abstract = {We introduce a computationally effective algorithm for a linear model selection consisting of three steps: screening--ordering--selection ({SOS).} Screening of predictors is based on the thresholded Lasso that is l\_1 penalized least squares. The screened predictors are then fitted using least squares ({LS)} and ordered with respect to their t statistics. Finally, a model is selected using greedy generalized information criterion ({GIC)} that is l\_0 penalized {LS} in a nested family induced by the ordering. We give non-asymptotic upper bounds on error probability of each step of the {SOS} algorithm in terms of both penalties. Then we obtain selection consistency for different (n, p) scenarios under conditions which are needed for screening consistency of the Lasso. For the traditional setting (n {\textgreater}p) we give Sanov-type bounds on the error probabilities of the ordering--selection algorithm. Its surprising consequence is that the selection error of greedy {GIC} is asymptotically not larger than of exhaustive {GIC.} We also obtain new bounds on prediction and estimation errors for the Lasso which are proved in parallel for the algorithm used in practice and its formal version.},
	number = {1310.6062},
	urldate = {2013-10-24},
	author = {Pokarowski, Piotr and Mielniczuk, Jan},
	month = oct,
	year = {2013},
	keywords = {Statistics - Machine Learning}
},

@techreport{zhang_spatial-spectral_2013,
	type = {{arXiv} e-print},
	title = {Spatial-Spectral Boosting Analysis for Stroke Patients' Motor Imagery {EEG} in Rehabilitation Training},
	url = {http://arxiv.org/abs/1310.6288},
	abstract = {Current studies about motor imagery based rehabilitation training systems for stroke subjects lack an appropriate analytic method, which can achieve a considerable classification accuracy, at the same time detects gradual changes of imagery patterns during rehabilitation process and disinters potential mechanisms about motor function recovery. In this study, we propose an adaptive boosting algorithm based on the cortex plasticity and spectral band shifts. This approach models the usually predetermined spatial-spectral configurations in {EEG} study into variable preconditions, and introduces a new heuristic of stochastic gradient boost for training base learners under these preconditions. We compare our proposed algorithm with commonly used methods on datasets collected from 2 months' clinical experiments. The simulation results demonstrate the effectiveness of the method in detecting the variations of stroke patients' {EEG} patterns. By chronologically reorganizing the weight parameters of the learned additive model, we verify the spatial compensatory mechanism on impaired cortex and detect the changes of accentuation bands in spectral domain, which may contribute important prior knowledge for rehabilitation practice.},
	number = {1310.6288},
	urldate = {2013-10-24},
	author = {Zhang, Hao and Zhang, Liqing},
	month = oct,
	year = {2013},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning}
},

@techreport{wang_universalities_2013,
	type = {{arXiv} e-print},
	title = {Universalities of Reproducing Kernels Revisited},
	url = {http://arxiv.org/abs/1310.5543},
	abstract = {Kernel methods have been widely applied to machine learning and other questions of approximating an unknown function from its finite sample data. To ensure arbitrary accuracy of such approximation, various denseness conditions are imposed on the selected kernel. This note contributes to the study of universal, characteristic, and {\$C\_0\$-universal} kernels. We first give simple and direct description of the difference and relation among these three kinds of universalities of kernels. We then focus on translation-invariant and weighted polynomial kernels. A simple and shorter proof of the known characterization of characteristic translation-invariant kernels will be presented. The main purpose of the note is to give a delicate discussion on the universalities of weighted polynomial kernels.},
	number = {1310.5543},
	urldate = {2013-10-24},
	author = {Wang, Benxun and Zhang, Haizhang},
	month = oct,
	year = {2013},
	keywords = {Statistics - Machine Learning}
},

@techreport{massam_distributed_2013,
	type = {{arXiv} e-print},
	title = {Distributed parameter estimation of discrete hierarchical models via marginal likelihoods},
	url = {http://arxiv.org/abs/1310.5666},
	abstract = {We consider discrete graphical models Markov with respect to a graph {\$G\$} and propose two distributed marginal methods to estimate the maximum likelihood estimate of the canonical parameter of the model. Both methods are based on a relaxation of the marginal likelihood obtained by considering the density of the variables represented by a vertex \$v\$ of {\$G\$} and a neighborhood. The two methods differ by the size of the neighborhood of \$v\$. We show that the estimates are consistent and that those obtained with the larger neighborhood have smaller asymptotic variance than the ones obtained through the smaller neighborhood.},
	number = {1310.5666},
	urldate = {2013-10-24},
	author = {Massam, Helene and Wang, Nanwei},
	month = oct,
	year = {2013},
	keywords = {{62H17} (Primary), {62M40}, Statistics - Machine Learning}
},

@techreport{gao_minimax_2013,
	type = {{arXiv} e-print},
	title = {Minimax Optimal Convergence Rates for Estimating Ground Truth from Crowdsourced Labels},
	url = {http://arxiv.org/abs/1310.5764},
	abstract = {Most machine learning challenges are essentially caused by insufficient amount of training data. In recent years, there is a rapid increase in the popularity of using crowdsourcing to collect labels for machine learning. With the emerging crowdsourcing services, we can obtain a large number of labels at a low cost from millions of crowdsourcing workers world wide. However, the labels provided by those non-expert crowdsourcing workers might not be of high quality. To fix this issue, in general, crowdsourcing requesters let each item be repeatedly labeled by several different workers and then estimating ground truth via some aggregation. In practice, Dawid-Skene estimator is widely adopted for this label aggregation purpose. It is a method proposed more than thirty years ago, but somehow we have not noticed any theoretic result on its convergence rate in the literature. In this paper, we establish minimax optimal convergence rates for Dawid-Skene estimator. We obtain a lower bound which holds for all estimators and an upper bound for Dawid-Skene estimator. We show that the upper bound matches the lower bound. Thus, Dawid-Skene estimator achieves the minimax optimality. Moreover, we conduct a comparative study of Dawid-Skene estimator and majority voting. We highlight the advantages and possible drawbacks of Dawid-Skene estimator through rigorous analysis in various settings.},
	number = {1310.5764},
	urldate = {2013-10-24},
	author = {Gao, Chao and Zhou, Dengyong},
	month = oct,
	year = {2013},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning}
},

@techreport{cai_rop:_2013,
	type = {{arXiv} e-print},
	title = {{ROP:} Matrix Recovery via Rank-One Projections},
	shorttitle = {{ROP}},
	url = {http://arxiv.org/abs/1310.5791},
	abstract = {Estimation of low-rank matrices is of significant interest in a range of contemporary applications. In this paper, we introduce a rank-one projection model for low-rank matrix recovery and propose a constrained nuclear norm minimization method for stable recovery of low-rank matrices in the noisy case. The procedure is adaptive to the rank and robust against small low-rank perturbations. Both upper and lower bounds for the estimation accuracy under the Frobenius norm loss are obtained. The proposed estimator is shown to be rate-optimal under certain conditions. The estimator is easy to implement via convex programming and performs well numerically. The main results obtained in the paper also have implications to other related statistical problems. An application to estimation of spike covariance matrices from one-dimensional random projections is considered. The results demonstrate that it is possible to accurately estimate the covariance matrix of a high-dimensional distribution based only on one-dimensional projections.},
	number = {1310.5791},
	urldate = {2013-10-24},
	author = {Cai, T. Tony and Zhang, Anru},
	month = oct,
	year = {2013},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology}
},

@techreport{needell_stochastic_2013,
	type = {{arXiv} e-print},
	title = {Stochastic gradient descent and the randomized Kaczmarz algorithm},
	url = {http://arxiv.org/abs/1310.5715},
	abstract = {We show that the exponential convergence rate of stochastic gradient descent for smooth strongly convex objectives can be markedly improved by perturbing the row selection rule in the direction of sampling estimates proportionally to the Lipschitz constants of their gradients. That is, we show that partially biased sampling allows a convergence rate with linear dependence on the average condition number of the system, compared to dependence on the average squared condition number for standard stochastic gradient descent. We assume the regime where all stochastic estimates share an optimum and so such an exponential rate is possible. We then recast the randomized Kaczmarz algorithm for solving overdetermined linear systems as an instance of preconditioned stochastic gradient descent, and apply our results to prove its exponential convergence, but to the solution of a weighted least squares problem rather than the original least squares problem. We present a modified Kaczmarz algorithm with partially biased sampling which does converge to the original least squares solution with the same exponential convergence rate.},
	number = {1310.5715},
	urldate = {2013-10-24},
	author = {Needell, Deanna and Srebro, Nathan and Ward, Rachel},
	month = oct,
	year = {2013},
	keywords = {{65B99}, {52A99}, {60G99}, {62L20}, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning}
},

@techreport{samek_multiple_2013,
	type = {{arXiv} e-print},
	title = {Multiple Kernel Learning for Brain-Computer Interfacing},
	url = {http://arxiv.org/abs/1310.6067},
	abstract = {Combining information from different sources is a common way to improve classification accuracy in Brain-Computer Interfacing ({BCI).} For instance, in small sample settings it is useful to integrate data from other subjects or sessions in order to improve the estimation quality of the spatial filters or the classifier. Since data from different subjects may show large variability, it is crucial to weight the contributions according to importance. Many multi-subject learning algorithms determine the optimal weighting in a separate step by using heuristics, however, without ensuring that the selected weights are optimal with respect to classification. In this work we apply Multiple Kernel Learning ({MKL)} to this problem. {MKL} has been widely used for feature fusion in computer vision and allows to simultaneously learn the classifier and the optimal weighting. We compare the {MKL} method to two baseline approaches and investigate the reasons for performance improvement.},
	number = {1310.6067},
	urldate = {2013-10-24},
	author = {Samek, Wojciech and Binder, Alexander and Müller, Klaus-Robert},
	month = oct,
	year = {2013},
	note = {W. Samek, A. Binder, K.-R. M{\textbackslash}"uller. Multiple Kernel Learning for Brain-Computer Interfacing. Proceedings of 35th Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society ({EMBC)}, 7048-51, 2013},
	keywords = {Statistics - Machine Learning}
},

@techreport{reece_efficient_2013,
	type = {{arXiv} e-print},
	title = {Efficient State-Space Inference of Periodic Latent Force Models},
	url = {http://arxiv.org/abs/1310.6319},
	abstract = {Latent force models ({LFM)} are principled approaches to incorporating solutions to differential equations within non-parametric inference methods. Unfortunately, the development and application of {LFMs} can be inhibited by their computational cost, especially when closed-form solutions for the {LFM} are unavailable, as is the case in many real world problems where these latent forces exhibit periodic behaviour. Given this, we develop a new sparse representation of {LFMs} which considerably improves their computational efficiency, as well as broadening their applicability, in a principled way, to domains with periodic or near periodic latent forces. Our approach uses a linear basis model to approximate one generative model for each periodic force. We assume that the latent forces are generated from Gaussian process priors and develop a linear basis model which fully expresses these priors. We apply our approach to model the thermal dynamics of real homes and show that it is effective in predicting day-ahead temperatures within the homes. We also apply our approach within queueing theory in which quasi-periodic arrival rates are modelled as latent forces. We demonstrate that our approach can be implemented efficiently using state-space methods which encode the linear dynamic systems via {LFMs.} Further, we show that state estimates obtained using periodic latent force models can reduce the root mean squared error to 59\% compared to estimates from non-periodic models and 84\% compared to the nearest rival approach which is the Resonator model.},
	number = {1310.6319},
	urldate = {2013-10-24},
	author = {Reece, Steven and Roberts, Stephen and Ghosh, Siddhartha and Rogers, Alex and Jennings, Nicholas},
	month = oct,
	year = {2013},
	keywords = {Statistics - Machine Learning}
}
